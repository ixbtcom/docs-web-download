#!/usr/bin/env python3
"""
–°–∫–∞—á–∏–≤–∞–Ω–∏–µ –æ–¥–Ω–æ–π –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü—ã –∏ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ Markdown.

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
    python3 fetch_single.py --parser docusaurus --url URL --output file.md
    python3 fetch_single.py --parser raw --url URL --output file.md
    python3 fetch_single.py --parser timeweb --url URL --output file.md
"""

import argparse
import re
import sys
from pathlib import Path

import requests
from bs4 import BeautifulSoup

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
                  "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
}


def fetch_raw(url: str) -> str | None:
    """–°–∫–∞—á–∏–≤–∞–µ—Ç raw markdown."""
    resp = requests.get(url, headers=HEADERS, timeout=30)
    resp.raise_for_status()
    return resp.text.strip()


def fetch_html(url: str, parser_type: str) -> str | None:
    """–°–∫–∞—á–∏–≤–∞–µ—Ç HTML-—Å—Ç—Ä–∞–Ω–∏—Ü—É –∏ –∫–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ—Ç –≤ markdown."""
    from markdownify import MarkdownConverter

    resp = requests.get(url, headers=HEADERS, timeout=30)
    resp.raise_for_status()
    soup = BeautifulSoup(resp.text, "lxml")

    h1 = soup.find("h1")
    title = h1.get_text().strip() if h1 else "Untitled"

    # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –ø–∞—Ä—Å–µ—Ä–∞
    if parser_type == "timeweb":
        article = soup.find(attrs={"itemprop": "articleBody"})
    else:  # docusaurus
        article = soup.find("article") or soup.find("main")

    if not article:
        print(f"   ‚ö†Ô∏è  –ö–æ–Ω—Ç–µ–Ω—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω")
        return None

    # –£–¥–∞–ª—è–µ–º –º—É—Å–æ—Ä
    for tag_name in ["nav", "footer", "aside"]:
        for el in article.find_all(tag_name):
            el.decompose()
    for btn in article.find_all("button"):
        btn.decompose()
    for el in article.find_all(string=re.compile(r"^On this page$")):
        parent = el.parent
        if parent and parent.name in ("div", "span", "aside", "li"):
            parent.decompose()

    # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º
    md = MarkdownConverter(
        heading_style="ATX", bullets="-", strong_em_symbol="*",
        strip=["script", "style", "noscript", "svg"],
    ).convert(str(article))

    # –ß–∏—Å—Ç–∫–∞
    md = re.sub(r"\n{3,}", "\n\n", md)
    md = "\n".join(line.rstrip() for line in md.split("\n"))
    md = md.strip()

    # –î–æ–±–∞–≤–ª—è–µ–º h1 –µ—Å–ª–∏ –Ω–µ—Ç
    if not md.startswith("# "):
        md = f"# {title}\n\n{md}"

    return md


def main():
    parser = argparse.ArgumentParser(description="–°–∫–∞—á–∞—Ç—å –æ–¥–Ω—É –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü—É –≤ Markdown")
    parser.add_argument("--parser", "-p", required=True,
                        choices=["docusaurus", "timeweb", "raw"],
                        help="–¢–∏–ø –ø–∞—Ä—Å–µ—Ä–∞")
    parser.add_argument("--url", "-u", required=True, help="URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã")
    parser.add_argument("--output", "-o", required=True, help="–ü—É—Ç—å –∫ –≤—ã—Ö–æ–¥–Ω–æ–º—É .md —Ñ–∞–π–ª—É")
    args = parser.parse_args()

    print(f"üìÑ –°–∫–∞—á–∏–≤–∞—é: {args.url}")

    try:
        if args.parser == "raw":
            content = fetch_raw(args.url)
        else:
            content = fetch_html(args.url, args.parser)
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}")
        sys.exit(1)

    if not content:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –∫–æ–Ω—Ç–µ–Ω—Ç")
        sys.exit(1)

    out_path = Path(args.output)
    out_path.parent.mkdir(parents=True, exist_ok=True)
    out_path.write_text(content + "\n", encoding="utf-8")
    print(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {out_path} ({len(content)} —Å–∏–º–≤–æ–ª–æ–≤)")


if __name__ == "__main__":
    main()
