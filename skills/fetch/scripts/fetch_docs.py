#!/usr/bin/env python3
"""
Ğ£Ğ½Ğ¸Ğ²ĞµÑ€ÑĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑĞºÑ€Ğ¸Ğ¿Ñ‚ Ğ´Ğ»Ñ ÑĞºĞ°Ñ‡Ğ¸Ğ²Ğ°Ğ½Ğ¸Ñ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ¸ ĞºĞ¾Ğ½Ğ²ĞµÑ€Ñ‚Ğ°Ñ†Ğ¸Ğ¸ Ğ² Markdown.

ĞŸĞ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµĞ¼Ñ‹Ğµ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¸:
  - timeweb-k8s  â€” Timeweb Cloud Kubernetes docs
  - jitsu         â€” Jitsu self-hosting docs (Docusaurus + GitHub raw)

Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ:
    python3 fetch_docs.py                          # Ğ²ÑĞµ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¸ â†’ ./docs/
    python3 fetch_docs.py jitsu                    # Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Jitsu â†’ ./docs/
    python3 fetch_docs.py --output /path/to/docs   # Ñ ÑƒĞºĞ°Ğ·Ğ°Ğ½Ğ¸ĞµĞ¼ Ğ²Ñ‹Ñ…Ğ¾Ğ´Ğ½Ğ¾Ğ¹ Ğ¿Ğ°Ğ¿ĞºĞ¸
    python3 fetch_docs.py jitsu --output /tmp/docs # ĞºĞ¾Ğ¼Ğ±Ğ¸Ğ½Ğ°Ñ†Ğ¸Ñ

Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚:
    {output}/{source}/*.md â€” ÑĞºĞ¾Ğ½Ğ²ĞµÑ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğµ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹
    {output}/{source}/images/{slug}/ â€” ÑĞºĞ°Ñ‡Ğ°Ğ½Ğ½Ñ‹Ğµ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ñ
    {output}/{source}/index.md â€” Ğ¾Ğ³Ğ»Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ
"""

import os
import re
import sys
import time
import hashlib
import urllib.parse
from pathlib import Path

import requests
from bs4 import BeautifulSoup, Tag
from markdownify import MarkdownConverter

# â”€â”€â”€ ĞĞ±Ñ‰Ğ¸Ğµ Ğ½Ğ°ÑÑ‚Ñ€Ğ¾Ğ¹ĞºĞ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

DOCS_DIR = Path.cwd() / "docs"  # Ğ¿Ğ¾ ÑƒĞ¼Ğ¾Ğ»Ñ‡Ğ°Ğ½Ğ¸Ñ â€” ./docs Ğ² Ñ‚ĞµĞºÑƒÑ‰ĞµĞ¹ Ğ´Ğ¸Ñ€ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ¸
DELAY = 1.0  # ÑĞµĞºÑƒĞ½Ğ´Ñ‹ Ğ¼ĞµĞ¶Ğ´Ñƒ Ğ·Ğ°Ğ¿Ñ€Ğ¾ÑĞ°Ğ¼Ğ¸

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
                  "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
}

# â”€â”€â”€ ĞŸÑ€Ğ¾Ñ„Ğ¸Ğ»Ğ¸ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ² â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

SOURCES = {
    "timeweb-k8s": {
        "base_url": "https://timeweb.cloud",
        "parser": "timeweb",
        "output_dir": "timeweb-kubernetes",
        "path_prefix": "/docs/k8s",
        "index_title": "Ğ”Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Timeweb Cloud â€” Kubernetes",
        "doc_paths": [
            "/docs/k8s",
            "/docs/k8s/create-cluster",
            "/docs/k8s/manage-cluster",
            "/docs/k8s/container-registry",
            "/docs/k8s/cluster-connection",
            "/docs/k8s/cluster-connection/kubectl",
            "/docs/k8s/cluster-connection/lens",
            "/docs/k8s/cluster-connection/freelens",
            "/docs/k8s/kubernetes-load-balancer",
            "/docs/k8s/kubernetes-autoscaling",
            "/docs/k8s/kubernetes-autoscaling/kubernetes-autoscaler",
            "/docs/k8s/kubernetes-autoscaling/autoscaling-to-zero-nodes",
            "/docs/k8s/network-drives-connection",
            "/docs/k8s/connect-oidc-provider-to-cluster",
            "/docs/k8s/helm",
            "/docs/k8s/helm-chart-creation",
            "/docs/k8s/network-plugins",
            "/docs/k8s/addons",
            "/docs/k8s/addons/nginx-ingress",
            "/docs/k8s/addons/openfaas",
            "/docs/k8s/addons/minio-operator",
            "/docs/k8s/addons/dbaas-operator",
            "/docs/k8s/addons/cert-manager",
            "/docs/k8s/addons/cert-manager-webhook",
            "/docs/k8s/addons/traefik",
            "/docs/k8s/addons/fluent-operator",
            "/docs/k8s/addons/velero",
            "/docs/k8s/addons/externaldns",
            "/docs/k8s/addons/grafana-loki",
            "/docs/k8s/addons/victoriametrics-operator",
            "/docs/k8s/addons/vault",
            "/docs/k8s/addons/wordpress",
            "/docs/k8s/addons/twc-alert-bot",
            "/docs/k8s/addons/apache-pulsar",
            "/docs/k8s/addons/cpa",
            "/docs/k8s/addons/csi-s3",
        ],
        "raw_urls": [],
    },
    "jitsu": {
        "base_url": "https://docs.jitsu.com",
        "parser": "docusaurus",
        "output_dir": "jitsu",
        "path_prefix": "",
        "index_title": "Ğ”Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Jitsu â€” Self-Hosting",
        "doc_paths": [
            "/self-hosting",
            "/self-hosting/quick-start",
            "/self-hosting/quick-start/syncs",
            "/self-hosting/production-deployment",
            "/self-hosting/configuration",
        ],
        "raw_urls": [
            # (URL, slug-Ğ¸Ğ¼Ñ Ñ„Ğ°Ğ¹Ğ»Ğ°)
            (
                "https://raw.githubusercontent.com/jitsucom/bulker/main/.docs/server-config.md",
                "bulker-server-config",
            ),
        ],
    },
}

# â”€â”€â”€ Ğ£Ñ‚Ğ¸Ğ»Ğ¸Ñ‚Ñ‹ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


def path_to_slug(doc_path: str, path_prefix: str) -> str:
    """ĞšĞ¾Ğ½Ğ²ĞµÑ€Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ URL path Ğ² Ğ¸Ğ¼Ñ Ñ„Ğ°Ğ¹Ğ»Ğ°.
    /docs/k8s/addons/nginx-ingress  (prefix=/docs/k8s) -> addons--nginx-ingress
    /self-hosting/configuration     (prefix=)           -> self-hosting--configuration
    """
    relative = doc_path.removeprefix(path_prefix).strip("/")
    if not relative:
        # ĞšĞ¾Ñ€Ğ½ĞµĞ²Ğ°Ñ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ğ° â€” Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ½Ğ¸Ğ¹ ÑĞµĞ³Ğ¼ĞµĞ½Ñ‚ prefix Ğ¸Ğ»Ğ¸ Ğ¿ÑƒÑ‚ÑŒ
        fallback = path_prefix.strip("/").split("/")[-1] if path_prefix else doc_path.strip("/")
        return fallback.replace("/", "--")
    return relative.replace("/", "--")


def download_image(img_url: str, slug: str, images_dir: Path,
                   base_url: str, session: requests.Session) -> str | None:
    """Ğ¡ĞºĞ°Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğµ Ğ¸ Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ¾Ñ‚Ğ½Ğ¾ÑĞ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ Ğ¿ÑƒÑ‚ÑŒ."""
    try:
        if img_url.startswith("//"):
            img_url = "https:" + img_url
        elif img_url.startswith("/"):
            img_url = base_url + img_url

        parsed = urllib.parse.urlparse(img_url)
        path_part = parsed.path.rstrip("/")
        filename = os.path.basename(path_part)
        if not filename or filename == "assets":
            filename = hashlib.md5(img_url.encode()).hexdigest()[:12]
        if "." not in filename:
            filename += ".png"

        img_dir = images_dir / slug
        img_dir.mkdir(parents=True, exist_ok=True)
        img_path = img_dir / filename

        if img_path.exists():
            return f"images/{slug}/{filename}"

        resp = session.get(img_url, headers=HEADERS, timeout=30)
        resp.raise_for_status()
        img_path.write_bytes(resp.content)
        print(f"    ğŸ“· {filename} ({len(resp.content) // 1024} KB)")
        return f"images/{slug}/{filename}"

    except Exception as e:
        print(f"    âš ï¸  ĞÑˆĞ¸Ğ±ĞºĞ° Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸ {img_url}: {e}")
        return None


def clean_markdown(md_content: str) -> str:
    """ĞĞ±Ñ‰Ğ°Ñ Ñ‡Ğ¸ÑÑ‚ĞºĞ° ÑĞºĞ¾Ğ½Ğ²ĞµÑ€Ñ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ğ¾Ğ³Ğ¾ Markdown."""
    md_content = re.sub(r"\n{3,}", "\n\n", md_content)
    md_content = "\n".join(line.rstrip() for line in md_content.split("\n"))
    md_content = md_content.strip()
    return md_content


def generate_index(pages: list[tuple[str, str]], title: str) -> str:
    """Ğ“ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµÑ‚ index.md Ñ Ğ¾Ğ³Ğ»Ğ°Ğ²Ğ»ĞµĞ½Ğ¸ĞµĞ¼."""
    lines = [f"# {title}", "", "ĞĞ³Ğ»Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ Ğ²ÑĞµÑ… ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ† Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ğ¸.", ""]

    sections: dict[str, list[tuple[str, str]]] = {}
    for slug, page_title in pages:
        parts = slug.split("--")
        section = parts[0] if len(parts) > 1 else ""
        sections.setdefault(section, []).append((slug, page_title))

    if "" in sections:
        for slug, page_title in sections[""]:
            lines.append(f"- [{page_title}]({slug}.md)")
        lines.append("")

    for section in sorted(sections.keys()):
        if section == "":
            continue
        lines.append(f"### {section.replace('-', ' ').title()}")
        lines.append("")
        for slug, page_title in sections[section]:
            lines.append(f"- [{page_title}]({slug}.md)")
        lines.append("")

    return "\n".join(lines) + "\n"


# â”€â”€â”€ ĞŸĞ°Ñ€ÑĞµÑ€: Timeweb Cloud â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


class TwcMarkdownConverter(MarkdownConverter):
    """ĞšĞ°ÑÑ‚Ğ¾Ğ¼Ğ½Ñ‹Ğ¹ ĞºĞ¾Ğ½Ğ²ĞµÑ€Ñ‚ĞµÑ€ Ğ´Ğ»Ñ HTML Timeweb Cloud."""

    def __init__(self, slug: str, images_dir: Path, base_url: str,
                 session: requests.Session, **kwargs):
        self.slug = slug
        self.images_dir = images_dir
        self.base_url = base_url
        self.session = session
        super().__init__(**kwargs)

    def convert_pre(self, el, text, **kwargs):
        code_el = el.find("code", attrs={"data-highlighted": "yes"})
        if not code_el:
            code_el = el.find("code")
        if not code_el:
            return f"\n```\n{text.strip()}\n```\n\n"

        lang = ""
        classes = code_el.get("class", [])
        if isinstance(classes, str):
            classes = classes.split()
        for cls in classes:
            if cls.startswith("language-"):
                lang = cls.removeprefix("language-")
                break

        code_text = code_el.get_text().replace("\xa0", " ")
        lines = [line.rstrip() for line in code_text.split("\n")]
        code_text = "\n".join(lines).strip()
        return f"\n```{lang}\n{code_text}\n```\n\n"

    def convert_code(self, el, text, **kwargs):
        parent = el.parent
        while parent:
            if parent.name == "pre":
                return text
            parent = parent.parent

        img = el.find("img")
        if img:
            return self.convert_img(img, "", **kwargs)

        code_text = el.get_text().replace("\xa0", " ").strip()
        if not code_text:
            return ""
        if "`" in code_text:
            return f"`` {code_text} ``"
        return f"`{code_text}`"

    def convert_img(self, el, text, **kwargs):
        src = el.get("src", "")
        alt = el.get("alt", "")
        if not src:
            return ""

        width = el.get("width")
        height = el.get("height")
        if width and height:
            try:
                if int(width) <= 100 and int(height) <= 100:
                    return ""
            except ValueError:
                pass

        local_path = download_image(src, self.slug, self.images_dir,
                                    self.base_url, self.session)
        if local_path:
            return f"![{alt}]({local_path})\n\n"
        return f"![{alt}]({src})\n\n"

    def convert_h2(self, el, text, **kwargs):
        return self._convert_heading(el, 2)

    def convert_h3(self, el, text, **kwargs):
        return self._convert_heading(el, 3)

    def convert_h4(self, el, text, **kwargs):
        return self._convert_heading(el, 4)

    def _convert_heading(self, el, level):
        for a in el.find_all("a", recursive=True):
            a.decompose()
        for svg in el.find_all("svg", recursive=True):
            svg.decompose()
        for div in el.find_all("div", recursive=True):
            div.decompose()

        text = el.get_text().strip()
        if not text:
            return ""
        hashes = "#" * level
        return f"\n{hashes} {text}\n\n"


def clean_twc_article(article_div: Tag) -> Tag:
    """Ğ£Ğ´Ğ°Ğ»ÑĞµÑ‚ Ğ½ĞµĞ½ÑƒĞ¶Ğ½Ñ‹Ğµ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ñ‹ Ğ¸Ğ· articleBody (Timeweb)."""
    for btn in article_div.find_all("div", class_=lambda c: c and "copyButton" in c):
        btn.decompose()
    for qr in article_div.find_all("div", class_=lambda c: c and "qr" in str(c).lower()):
        qr.decompose()
    for el in article_div.find_all(string=re.compile(r"Ğ‘Ñ‹Ğ»Ğ° Ğ»Ğ¸ ÑÑ‚Ğ°Ñ‚ÑŒÑ Ğ¿Ğ¾Ğ»ĞµĞ·Ğ½Ğ°")):
        node = el.parent
        while node and node != article_div:
            classes = " ".join(node.get("class", []))
            if "twCard" in classes or (node.name == "section" and node.parent == article_div):
                node.decompose()
                break
            node = node.parent
        else:
            h5 = el.find_parent("h5")
            if h5:
                for sibling in list(h5.find_next_siblings()):
                    sibling.decompose()
                h5.decompose()
    return article_div


def fetch_timeweb(doc_path: str, source_cfg: dict,
                  out_dir: Path, session: requests.Session) -> tuple[str, str] | None:
    """Ğ¡ĞºĞ°Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¸ ĞºĞ¾Ğ½Ğ²ĞµÑ€Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñƒ Timeweb Cloud."""
    base_url = source_cfg["base_url"]
    url = base_url + doc_path
    slug = path_to_slug(doc_path, source_cfg["path_prefix"])
    images_dir = out_dir / "images"

    print(f"\nğŸ“„ {slug} â€” {url}")

    try:
        resp = session.get(url, headers=HEADERS, timeout=30)
        resp.raise_for_status()
    except Exception as e:
        print(f"   âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸: {e}")
        return None

    soup = BeautifulSoup(resp.text, "lxml")
    h1 = soup.find("h1")
    title = h1.get_text().strip() if h1 else slug

    article = soup.find(attrs={"itemprop": "articleBody"})
    if not article:
        print(f"   âš ï¸  articleBody Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½, Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ°ĞµĞ¼")
        return None

    article = clean_twc_article(article)

    converter = TwcMarkdownConverter(
        slug=slug, images_dir=images_dir, base_url=base_url, session=session,
        heading_style="ATX", bullets="-", strong_em_symbol="*",
        strip=["script", "style", "svg", "noscript"],
    )
    md_content = converter.convert(str(article))
    md_content = clean_markdown(md_content)
    md_content = re.sub(r"\n*ĞŸĞ¾ĞºĞ° Ğ½ĞµÑ‚ ĞºĞ¾Ğ¼Ğ¼ĞµĞ½Ñ‚Ğ°Ñ€Ğ¸ĞµĞ²\s*$", "", md_content)
    md_content = md_content.strip()

    return slug, f"# {title}\n\n{md_content}\n"


# â”€â”€â”€ ĞŸĞ°Ñ€ÑĞµÑ€: Docusaurus â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


class DocusaurusMarkdownConverter(MarkdownConverter):
    """ĞšĞ¾Ğ½Ğ²ĞµÑ€Ñ‚ĞµÑ€ Ğ´Ğ»Ñ Docusaurus-ÑĞ°Ğ¹Ñ‚Ğ¾Ğ² (docs.jitsu.com Ğ¸ Ñ‚.Ğ¿.)."""

    def __init__(self, slug: str, images_dir: Path, base_url: str,
                 session: requests.Session, **kwargs):
        self.slug = slug
        self.images_dir = images_dir
        self.base_url = base_url
        self.session = session
        super().__init__(**kwargs)

    def convert_pre(self, el, text, **kwargs):
        code_el = el.find("code")
        if not code_el:
            return f"\n```\n{text.strip()}\n```\n\n"

        lang = ""
        classes = code_el.get("class", [])
        if isinstance(classes, str):
            classes = classes.split()
        for cls in classes:
            if cls.startswith("language-"):
                lang = cls.removeprefix("language-")
                break

        code_text = code_el.get_text().replace("\xa0", " ")
        lines = [line.rstrip() for line in code_text.split("\n")]
        code_text = "\n".join(lines).strip()
        return f"\n```{lang}\n{code_text}\n```\n\n"

    def convert_code(self, el, text, **kwargs):
        parent = el.parent
        while parent:
            if parent.name == "pre":
                return text
            parent = parent.parent

        code_text = el.get_text().replace("\xa0", " ").strip()
        if not code_text:
            return ""
        if "`" in code_text:
            return f"`` {code_text} ``"
        return f"`{code_text}`"

    def convert_img(self, el, text, **kwargs):
        src = el.get("src", "")
        alt = el.get("alt", "")
        if not src:
            return ""

        local_path = download_image(src, self.slug, self.images_dir,
                                    self.base_url, self.session)
        if local_path:
            return f"![{alt}]({local_path})\n\n"
        return f"![{alt}]({src})\n\n"

    def convert_h1(self, el, text, **kwargs):
        return self._convert_heading(el, 1)

    def convert_h2(self, el, text, **kwargs):
        return self._convert_heading(el, 2)

    def convert_h3(self, el, text, **kwargs):
        return self._convert_heading(el, 3)

    def convert_h4(self, el, text, **kwargs):
        return self._convert_heading(el, 4)

    def _convert_heading(self, el, level):
        # Ğ£Ğ´Ğ°Ğ»ÑĞµĞ¼ anchor-ÑÑÑ‹Ğ»ĞºĞ¸ Docusaurus
        for a in el.find_all("a", class_=lambda c: c and "hash-link" in c):
            a.decompose()
        for a in el.find_all("a", attrs={"aria-hidden": "true"}):
            a.decompose()
        for svg in el.find_all("svg", recursive=True):
            svg.decompose()

        text = el.get_text().strip()
        if not text:
            return ""
        hashes = "#" * level
        return f"\n{hashes} {text}\n\n"


def clean_docusaurus_article(article: Tag) -> Tag:
    """Ğ£Ğ´Ğ°Ğ»ÑĞµÑ‚ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ñ Ğ¸ ÑĞ»ÑƒĞ¶ĞµĞ±Ğ½Ñ‹Ğµ ÑĞ»ĞµĞ¼ĞµĞ½Ñ‚Ñ‹ Docusaurus."""
    # Ğ£Ğ´Ğ°Ğ»ÑĞµĞ¼ "On this page" ToC-Ğ·Ğ°Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğº (Ğ¾Ğ±Ñ‹Ñ‡Ğ½Ğ¾ Ğ² aside Ğ¸Ğ»Ğ¸ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ğ¾Ğ¼ div)
    for el in article.find_all(string=re.compile(r"^On this page$")):
        parent = el.parent
        if parent and parent.name in ("div", "span", "aside", "li"):
            parent.decompose()
        elif parent:
            el.extract()
    # Ğ£Ğ´Ğ°Ğ»ÑĞµĞ¼ Ğ½Ğ°Ğ²Ğ¸Ğ³Ğ°Ñ†Ğ¸Ñ prev/next
    for nav in article.find_all("nav", class_=lambda c: c and "pagination" in str(c)):
        nav.decompose()
    # Ğ£Ğ´Ğ°Ğ»ÑĞµĞ¼ ĞºĞ½Ğ¾Ğ¿ĞºĞ¸ "Edit this page"
    for a in article.find_all("a", string=re.compile(r"Edit this page|Ğ ĞµĞ´Ğ°ĞºÑ‚Ğ¸Ñ€Ğ¾Ğ²Ğ°Ñ‚ÑŒ")):
        parent = a.parent
        if parent and parent.name in ("div", "footer"):
            parent.decompose()
        else:
            a.decompose()
    # Ğ£Ğ´Ğ°Ğ»ÑĞµĞ¼ ĞºĞ½Ğ¾Ğ¿ĞºĞ¸ ĞºĞ¾Ğ¿Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ Ğ² Ğ±Ğ»Ğ¾ĞºĞ°Ñ… ĞºĞ¾Ğ´Ğ°
    for btn in article.find_all("button", class_=lambda c: c and "copy" in str(c).lower()):
        btn.decompose()
    # Ğ£Ğ´Ğ°Ğ»ÑĞµĞ¼ breadcrumbs
    for bc in article.find_all("nav", attrs={"aria-label": "Breadcrumbs"}):
        bc.decompose()
    # Ğ£Ğ´Ğ°Ğ»ÑĞµĞ¼ ToC sidebar ĞµÑĞ»Ğ¸ Ğ¿Ğ¾Ğ¿Ğ°Ğ» Ğ² article
    for toc in article.find_all("div", class_=lambda c: c and "tableOfContents" in str(c)):
        toc.decompose()
    # Ğ£Ğ´Ğ°Ğ»ÑĞµĞ¼ footer
    for footer in article.find_all("footer"):
        footer.decompose()
    return article


def fetch_docusaurus(doc_path: str, source_cfg: dict,
                     out_dir: Path, session: requests.Session) -> tuple[str, str] | None:
    """Ğ¡ĞºĞ°Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¸ ĞºĞ¾Ğ½Ğ²ĞµÑ€Ñ‚Ğ¸Ñ€ÑƒĞµÑ‚ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñƒ Docusaurus."""
    base_url = source_cfg["base_url"]
    url = base_url + doc_path
    slug = path_to_slug(doc_path, source_cfg["path_prefix"])
    images_dir = out_dir / "images"

    print(f"\nğŸ“„ {slug} â€” {url}")

    try:
        resp = session.get(url, headers=HEADERS, timeout=30)
        resp.raise_for_status()
    except Exception as e:
        print(f"   âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸: {e}")
        return None

    soup = BeautifulSoup(resp.text, "lxml")

    # Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµĞ¼ Ğ·Ğ°Ğ³Ğ¾Ğ»Ğ¾Ğ²Ğ¾Ğº
    h1 = soup.find("h1")
    title = h1.get_text().strip() if h1 else slug

    # Ğ˜Ñ‰ĞµĞ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚: <article>, Ğ¸Ğ»Ğ¸ <main>, Ğ¸Ğ»Ğ¸ .docMainContainer
    article = soup.find("article")
    if not article:
        article = soup.find("main")
    if not article:
        article = soup.find("div", class_=lambda c: c and "docMainContainer" in str(c))
    if not article:
        print(f"   âš ï¸  ĞšĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½, Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ°ĞµĞ¼")
        return None

    article = clean_docusaurus_article(article)

    converter = DocusaurusMarkdownConverter(
        slug=slug, images_dir=images_dir, base_url=base_url, session=session,
        heading_style="ATX", bullets="-", strong_em_symbol="*",
        strip=["script", "style", "noscript"],
    )
    md_content = converter.convert(str(article))
    md_content = clean_markdown(md_content)

    # Ğ£Ğ´Ğ°Ğ»ÑĞµĞ¼ Ğ´ÑƒĞ±Ğ»Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½Ñ‹Ğ¹ h1 (Docusaurus Ğ¸Ğ½Ğ¾Ğ³Ğ´Ğ° Ñ€ĞµĞ½Ğ´ĞµÑ€Ğ¸Ñ‚ ĞµĞ³Ğ¾ Ğ´Ğ²Ğ°Ğ¶Ğ´Ñ‹)
    lines = md_content.split("\n")
    if len(lines) > 2 and lines[0].startswith("# ") and lines[2].startswith("# "):
        lines = lines[2:]
        md_content = "\n".join(lines)

    # Ğ•ÑĞ»Ğ¸ h1 ÑƒĞ¶Ğµ ĞµÑÑ‚ÑŒ Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞ½Ñ‚Ğµ, Ğ½Ğµ Ğ´ÑƒĞ±Ğ»Ğ¸Ñ€ÑƒĞµĞ¼
    first_line = md_content.split("\n")[0].strip() if md_content else ""
    if first_line.startswith("# "):
        return slug, f"{md_content}\n"

    return slug, f"# {title}\n\n{md_content}\n"


# â”€â”€â”€ ĞŸĞ°Ñ€ÑĞµÑ€: Raw Markdown (GitHub Ğ¸ Ñ‚.Ğ¿.) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


def fetch_raw_markdown(url: str, slug: str,
                       session: requests.Session) -> tuple[str, str] | None:
    """Ğ¡ĞºĞ°Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ raw Markdown Ñ„Ğ°Ğ¹Ğ»."""
    print(f"\nğŸ“„ {slug} â€” {url}")

    try:
        resp = session.get(url, headers=HEADERS, timeout=30)
        resp.raise_for_status()
    except Exception as e:
        print(f"   âŒ ĞÑˆĞ¸Ğ±ĞºĞ° Ğ·Ğ°Ğ³Ñ€ÑƒĞ·ĞºĞ¸: {e}")
        return None

    md_content = resp.text.strip()
    return slug, f"{md_content}\n"


# â”€â”€â”€ Ğ”Ğ¸ÑĞ¿ĞµÑ‚Ñ‡ĞµÑ€ Ğ¿Ğ°Ñ€ÑĞµÑ€Ğ¾Ğ² â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

PARSERS = {
    "timeweb": fetch_timeweb,
    "docusaurus": fetch_docusaurus,
}


# â”€â”€â”€ ĞÑĞ½Ğ¾Ğ²Ğ½Ğ°Ñ Ğ»Ğ¾Ğ³Ğ¸ĞºĞ° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


def fetch_source(source_name: str) -> None:
    """Ğ¡ĞºĞ°Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ²ÑÑ Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ´Ğ»Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ°."""
    cfg = SOURCES[source_name]
    parser_fn = PARSERS[cfg["parser"]]
    out_dir = DOCS_DIR / cfg["output_dir"]
    out_dir.mkdir(parents=True, exist_ok=True)
    (out_dir / "images").mkdir(exist_ok=True)

    session = requests.Session()
    pages: list[tuple[str, str]] = []
    doc_paths = cfg["doc_paths"]
    raw_urls = cfg.get("raw_urls", [])
    total = len(doc_paths) + len(raw_urls)

    print(f"\nğŸš€ [{source_name}] Ğ¡ĞºĞ°Ñ‡Ğ¸Ğ²Ğ°Ğ½Ğ¸Ğµ {total} ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ† â†’ {out_dir}/\n")

    # HTML-ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹
    for i, doc_path in enumerate(doc_paths):
        result = parser_fn(doc_path, cfg, out_dir, session)
        if result:
            slug, md_content = result
            title = md_content.split("\n")[0].removeprefix("# ").strip()
            pages.append((slug, title))

            md_path = out_dir / f"{slug}.md"
            md_path.write_text(md_content, encoding="utf-8")
            print(f"   âœ… Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¾: {md_path.name} ({len(md_content)} ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ²)")

        if i < len(doc_paths) - 1:
            time.sleep(DELAY)

    # Raw Markdown URL-Ñ‹
    for raw_url, slug in raw_urls:
        if doc_paths:
            time.sleep(DELAY)
        result = fetch_raw_markdown(raw_url, slug, session)
        if result:
            slug, md_content = result
            title = md_content.split("\n")[0].removeprefix("# ").strip()
            pages.append((slug, title))

            md_path = out_dir / f"{slug}.md"
            md_path.write_text(md_content, encoding="utf-8")
            print(f"   âœ… Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¾: {md_path.name} ({len(md_content)} ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ²)")

    # Ğ“ĞµĞ½ĞµÑ€Ğ¸Ñ€ÑƒĞµĞ¼ index
    if pages:
        index_content = generate_index(pages, cfg["index_title"])
        index_path = out_dir / "index.md"
        index_path.write_text(index_content, encoding="utf-8")
        print(f"\nğŸ“‹ ĞĞ³Ğ»Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ğµ: {index_path}")

    # ĞŸĞ¾Ğ´ÑÑ‡Ñ‘Ñ‚ Ğ¸Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹
    images_dir = out_dir / "images"
    img_count = sum(1 for _ in images_dir.rglob("*") if _.is_file()) if images_dir.exists() else 0

    print(f"\nâœ¨ [{source_name}] Ğ“Ğ¾Ñ‚Ğ¾Ğ²Ğ¾!")
    print(f"   ğŸ“„ Ğ¡Ñ‚Ñ€Ğ°Ğ½Ğ¸Ñ†: {len(pages)}/{total}")
    print(f"   ğŸ–¼ï¸  Ğ˜Ğ·Ğ¾Ğ±Ñ€Ğ°Ğ¶ĞµĞ½Ğ¸Ğ¹: {img_count}")
    print(f"   ğŸ“ Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚: {out_dir}")


def main():
    global DOCS_DIR

    args = sys.argv[1:]
    names = []

    # ĞŸĞ°Ñ€ÑĞ¸Ğ¼ Ğ°Ñ€Ğ³ÑƒĞ¼ĞµĞ½Ñ‚Ñ‹
    i = 0
    while i < len(args):
        if args[i] == "--output" and i + 1 < len(args):
            DOCS_DIR = Path(args[i + 1])
            i += 2
        elif args[i].startswith("--"):
            print(f"âŒ ĞĞµĞ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ñ‹Ğ¹ Ñ„Ğ»Ğ°Ğ³: {args[i]}")
            sys.exit(1)
        else:
            names.append(args[i])
            i += 1

    # Ğ’Ğ°Ğ»Ğ¸Ğ´Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ¼Ñ‘Ğ½ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¾Ğ²
    for name in names:
        if name not in SOURCES:
            print(f"âŒ ĞĞµĞ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ñ‹Ğ¹ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸Ğº: {name}")
            print(f"   Ğ”Ğ¾ÑÑ‚ÑƒĞ¿Ğ½Ñ‹Ğµ: {', '.join(SOURCES.keys())}")
            sys.exit(1)

    if not names:
        names = list(SOURCES.keys())

    for name in names:
        fetch_source(name)


if __name__ == "__main__":
    main()
